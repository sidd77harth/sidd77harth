#Importing the required Libraries import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris data = load_iris()

X=data.data Y=data.target

#Code for randomly shuffling the dataset from scipy.sparse import coo_matrix X_sparse = coo_matrix(X)
from sklearn.utils import shuffle
X, X_sparse, y = shuffle(X, X_sparse, Y, random_state=0) X=X_sparse.toarray()
Y=y Y=Y.reshape(150,1)
 
#Divivding the dataset into training and testing train_X=X[0:125,:]
train_Y=Y[0:125,:] test_X=X[125:,:]
test_Y=Y[125:,:]

def sigmoid(x):
for i in range(0,x.shape[0]): x[i]=1/(1+np.exp(-1*x[i]))
return x
def diffSigmoid(x):
for i in range(0,len(x)): y=1/(1+np.exp(-1*x[i])) x[i]=y*(1-y)
return x

#Random initialization of Weights Matrices
#Model contains 4 layers having 4->8->8->3 neurons respectively w12=np.random.rand(8,4)
b1=np.random.rand(8,1) w23=np.random.rand(8,8) b2=np.random.rand(8,1) w34=np.random.rand(3,8) b3=np.random.rand(3,1)

def forwardPropagation(X,Y,dict2): dict ={}
w12=dict2["w12"] w23=dict2["w23"] w34=dict2["w34"] b1=dict2["b1"] b2=dict2["b2"] b3=dict2["b3"] X=np.asarray(X) X=X.reshape((4,1)) net1=np.dot(w12,X)+b1 a1=sigmoid(net1) net2=np.dot(w23,a1)+b2 a2=sigmoid(net2) net3=np.dot(w34,a2)+b3
 
a3=sigmoid(net3) error=1/2*np.sum(pow((Y-a3),2)) dict["a1"]=a1
dict["a2"]=a2
dict["a3"]=a3 dict["net1"]=net1 dict["net2"]=net2 dict["net3"]=net3 dict["err"]=error return dict

def Backpropagation(X,y,alpha,dict,dict2): a1=dict["a1"]
a2=dict["a2"]
a3=dict["a3"] net1=dict["net1"] net2=dict["net2"] net3=dict["net3"] error=dict["err"] w12=dict2["w12"] w23=dict2["w23"] w34=dict2["w34"] b1=dict2["b1"] b2=dict2["b2"] b3=dict2["b3"] y1=np.zeros((3,1)) y1[y]=1
a3=a3.reshape((3,1)) a2=a2.reshape((8,1)) a1=a1.reshape((8,1)) X=np.asarray(X) X=X.reshape((4,1))
temp=np.multiply(a3-y1,np.multiply(a3,np.ones(a3.shape)-a3))

temp2=np.multiply(np.dot(temp.transpose(),w34).T,np.multiply(a2,np.ones(a2.shape)-a2)
)

temp3=np.multiply(np.dot(temp2.transpose(),w23).T,np.multiply(a1,np.ones(a1.shape)-a 1))
w34=w34-(alpha)*(np.dot(temp,a2.T))
 
b3=b3-(alpha)*temp
w23=w23-(alpha)*(np.dot(temp2,a1.T)) b2=b2-(alpha)*temp2
w12=w12-(alpha)*(np.dot(temp3,X.T)) b1=b1-(alpha)*(temp3) dict2["w12"]=w12
dict2["w23"]=w23 dict2["w34"]=w34 dict2["b1"]=b1 dict2["b2"]=b2 dict2["b3"]=b3 return dict2

def main(X,y): max_iter=10 alpha=0.01 weights={} cost_list=[] weights["w12"]=w12 weights["w23"]=w23 weights["w34"]=w34 weights["b1"]=b1 weights["b2"]=b2 weights["b3"]=b3
weights['No_of_Iter']=max_iter while(True):
for i in range(1,X.shape[0]): d1=forwardPropagation(X[i],y[i],weights) weights=Backpropagation(X[i],y[i],alpha,d1,weights) curr_error=d1["err"]
max_iter=max_iter-1 if(max_iter<0):
break final=d1["err"] cost_list.append(final)
weights['cost']=cost_list return weights

ans=main(train_X,train_Y) u=ans["w23"]
 

#Prediction
from sklearn.metrics import mean_squared_error pred=forwardPropagation(test_X[19],test_Y[19],ans);

print(pred["a3"]) print(test_Y[19])

test_X.shape

i=ans['No_of_Iter'] iter=np.arange(0,i) error=ans['cost'] plt.plot(iter,error) plt.xlabel('No_of_iterations') plt.ylabel('Error')
plt.show
